{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COSMIC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMhaWQ87JUjvvxlLENLEbT9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tae898/conv-emotion/blob/master/COSMIC/erc-training/notebooks/COSMIC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spKTMrud2vpX"
      },
      "source": [
        "# Download the pre-processed features\n",
        "\n",
        "wget doesn't work so well with google drive links. Let's use [gdown](https://pypi.org/project/gdown/).\n",
        "\n",
        "The data is 5 GB. It might take some time to download."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9t4yKSb2yl9"
      },
      "source": [
        "import gdown\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1TQYQYCoPtdXN2rQ1mR2jisjUztmOzfZr&export=download'\n",
        "output = 'erc-training.zip'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5F83ntrEjg1"
      },
      "source": [
        "# Unzip the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggbzYu2nEhuU",
        "outputId": "328429f5-29a8-4a06-d720-9584062f1218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        }
      },
      "source": [
        "!unzip erc-training.zip\n",
        "!rm erc-training.zip\n",
        "!mv erc-training/* ./"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  erc-training.zip\n",
            "   creating: erc-training/\n",
            "  inflating: __MACOSX/._erc-training  \n",
            "  inflating: erc-training/.DS_Store  \n",
            "  inflating: __MACOSX/erc-training/._.DS_Store  \n",
            "   creating: erc-training/dailydialog/\n",
            "  inflating: __MACOSX/erc-training/._dailydialog  \n",
            "   creating: erc-training/iemocap/\n",
            "  inflating: __MACOSX/erc-training/._iemocap  \n",
            "   creating: erc-training/meld/\n",
            "  inflating: __MACOSX/erc-training/._meld  \n",
            "   creating: erc-training/emorynlp/\n",
            "  inflating: __MACOSX/erc-training/._emorynlp  \n",
            "  inflating: erc-training/dailydialog/.DS_Store  \n",
            "  inflating: __MACOSX/erc-training/dailydialog/._.DS_Store  \n",
            "  inflating: erc-training/dailydialog/dailydialog_features_roberta.pkl  \n",
            "  inflating: __MACOSX/erc-training/dailydialog/._dailydialog_features_roberta.pkl  \n",
            "  inflating: erc-training/dailydialog/dailydialog_features_comet.pkl  \n",
            "  inflating: __MACOSX/erc-training/dailydialog/._dailydialog_features_comet.pkl  \n",
            "  inflating: erc-training/iemocap/iemocap_features_roberta.pkl  \n",
            "  inflating: __MACOSX/erc-training/iemocap/._iemocap_features_roberta.pkl  \n",
            "  inflating: erc-training/iemocap/.DS_Store  \n",
            "  inflating: __MACOSX/erc-training/iemocap/._.DS_Store  \n",
            "  inflating: erc-training/iemocap/iemocap_features_comet.pkl  \n",
            "  inflating: __MACOSX/erc-training/iemocap/._iemocap_features_comet.pkl  \n",
            "  inflating: erc-training/meld/.DS_Store  \n",
            "  inflating: __MACOSX/erc-training/meld/._.DS_Store  \n",
            "  inflating: erc-training/meld/meld_features_comet.pkl  \n",
            "  inflating: __MACOSX/erc-training/meld/._meld_features_comet.pkl  \n",
            "  inflating: erc-training/meld/meld_features_roberta.pkl  \n",
            "  inflating: __MACOSX/erc-training/meld/._meld_features_roberta.pkl  \n",
            "  inflating: erc-training/emorynlp/.DS_Store  \n",
            "  inflating: __MACOSX/erc-training/emorynlp/._.DS_Store  \n",
            "  inflating: erc-training/emorynlp/emorynlp_features_comet.pkl  \n",
            "  inflating: __MACOSX/erc-training/emorynlp/._emorynlp_features_comet.pkl  \n",
            "  inflating: erc-training/emorynlp/emorynlp_features_roberta.pkl  \n",
            "  inflating: __MACOSX/erc-training/emorynlp/._emorynlp_features_roberta.pkl  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Aq5761rHWe4",
        "outputId": "7b89e9c9-c120-40f4-91f9-a9bdc7ccdd46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/tae898/conv-emotion/master/COSMIC/erc-training/commonsense_model.py\n",
        "!wget https://raw.githubusercontent.com/tae898/conv-emotion/master/COSMIC/erc-training/dataloader.py\n",
        "!wget https://raw.githubusercontent.com/tae898/conv-emotion/master/COSMIC/erc-training/model.py"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-11 14:59:29--  https://raw.githubusercontent.com/tae898/conv-emotion/master/COSMIC/erc-training/commonsense_model.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14415 (14K) [text/plain]\n",
            "Saving to: ‘commonsense_model.py’\n",
            "\n",
            "\rcommonsense_model.p   0%[                    ]       0  --.-KB/s               \rcommonsense_model.p 100%[===================>]  14.08K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2020-10-11 14:59:29 (1.96 MB/s) - ‘commonsense_model.py’ saved [14415/14415]\n",
            "\n",
            "--2020-10-11 14:59:29--  https://raw.githubusercontent.com/tae898/conv-emotion/master/COSMIC/erc-training/dataloader.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9390 (9.2K) [text/plain]\n",
            "Saving to: ‘dataloader.py’\n",
            "\n",
            "dataloader.py       100%[===================>]   9.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-10-11 14:59:29 (108 MB/s) - ‘dataloader.py’ saved [9390/9390]\n",
            "\n",
            "--2020-10-11 14:59:29--  https://raw.githubusercontent.com/tae898/conv-emotion/master/COSMIC/erc-training/model.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9419 (9.2K) [text/plain]\n",
            "Saving to: ‘model.py’\n",
            "\n",
            "model.py            100%[===================>]   9.20K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-10-11 14:59:30 (81.1 MB/s) - ‘model.py’ saved [9419/9419]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htxZVs3MLHcY"
      },
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import time\n",
        "import pickle\n",
        "import random\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import functional as F\n",
        "from dataloader import MELDRobertaCometDataset\n",
        "from model import MaskedNLLLoss\n",
        "from commonsense_model import CommonsenseGRUModel\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "\n",
        "def create_class_weight(mu=1):\n",
        "    unique = [0, 1, 2, 3, 4, 5, 6]\n",
        "    labels_dict = {0: 6436, 1: 1636, 2: 358, 3: 1002, 4: 2308, 5: 361, 6: 1607}\n",
        "    total = np.sum(list(labels_dict.values()))\n",
        "    weights = []\n",
        "    for key in unique:\n",
        "        score = math.log(mu*total/labels_dict[key])\n",
        "        weights.append(score)\n",
        "    return weights\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def get_MELD_loaders(batch_size=32, classify='emotion', num_workers=0, pin_memory=False):\n",
        "    trainset = MELDRobertaCometDataset('train', classify)\n",
        "    validset = MELDRobertaCometDataset('valid', classify)\n",
        "    testset = MELDRobertaCometDataset('test', classify)\n",
        "\n",
        "    train_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "\n",
        "    valid_loader = DataLoader(validset,\n",
        "                              batch_size=batch_size,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "\n",
        "    test_loader = DataLoader(testset,\n",
        "                             batch_size=batch_size,\n",
        "                             collate_fn=testset.collate_fn,\n",
        "                             num_workers=num_workers,\n",
        "                             pin_memory=pin_memory)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader\n",
        "\n",
        "\n",
        "def train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\n",
        "    losses, preds, labels, masks, losses_sense = [], [], [], [], []\n",
        "    alphas, alphas_f, alphas_b, vids = [], [], [], []\n",
        "    max_sequence_len = []\n",
        "\n",
        "    assert not train or optimizer != None\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    seed_everything(seed)\n",
        "    for data in dataloader:\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        r1, r2, r3, r4, \\\n",
        "            x1, x2, x3, x4, x5, x6, \\\n",
        "            o1, o2, o3, \\\n",
        "            qmask, umask, label = [d.cuda()\n",
        "                                   for d in data[:-1]] if cuda else data[:-1]\n",
        "\n",
        "        log_prob, _, alpha, alpha_f, alpha_b, _ = model(\n",
        "            r1, r2, r3, r4, x5, x6, x1, o2, o3, qmask, umask, att2=False)\n",
        "\n",
        "        lp_ = log_prob.transpose(0, 1).contiguous(\n",
        "        ).view(-1, log_prob.size()[2])  # batch*seq_len, n_classes\n",
        "        labels_ = label.view(-1)  # batch*seq_len\n",
        "        loss = loss_function(lp_, labels_, umask)\n",
        "\n",
        "        pred_ = torch.argmax(lp_, 1)  # batch*seq_len\n",
        "        preds.append(pred_.data.cpu().numpy())\n",
        "        labels.append(labels_.data.cpu().numpy())\n",
        "        masks.append(umask.view(-1).cpu().numpy())\n",
        "        losses.append(loss.item()*masks[-1].sum())\n",
        "\n",
        "        if train:\n",
        "            total_loss = loss\n",
        "            total_loss.backward()\n",
        "            if args.tensorboard:\n",
        "                for param in model.named_parameters():\n",
        "                    writer.add_histogram(param[0], param[1].grad, epoch)\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            alphas += alpha\n",
        "            alphas_f += alpha_f\n",
        "            alphas_b += alpha_b\n",
        "            vids += data[-1]\n",
        "\n",
        "    if preds != []:\n",
        "        preds = np.concatenate(preds)\n",
        "        labels = np.concatenate(labels)\n",
        "        masks = np.concatenate(masks)\n",
        "    else:\n",
        "        return float('nan'), float('nan'), float('nan'), [], [], [], float('nan'), []\n",
        "\n",
        "    avg_loss = round(np.sum(losses)/np.sum(masks), 4)\n",
        "    avg_sense_loss = round(np.sum(losses_sense)/np.sum(masks), 4)\n",
        "\n",
        "    avg_accuracy = round(accuracy_score(\n",
        "        labels, preds, sample_weight=masks)*100, 2)\n",
        "    avg_fscore = round(\n",
        "        f1_score(labels, preds, sample_weight=masks, average='weighted')*100, 2)\n",
        "    return avg_loss, avg_accuracy, labels, preds, masks, [avg_fscore], [alphas, alphas_f, alphas_b, vids]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB5rp_4cLcG-"
      },
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--no-cuda', action='store_true',\n",
        "                        default=False, help='does not use GPU')\n",
        "    parser.add_argument('--lr', type=float, default=0.0001,\n",
        "                        metavar='LR', help='learning rate')\n",
        "    parser.add_argument('--l2', type=float, default=0.0003,\n",
        "                        metavar='L2', help='L2 regularization weight')\n",
        "    parser.add_argument('--rec-dropout', type=float, default=0.5,\n",
        "                        metavar='rec_dropout', help='rec_dropout rate')\n",
        "    parser.add_argument('--dropout', type=float, default=0.5,\n",
        "                        metavar='dropout', help='dropout rate')\n",
        "    parser.add_argument('--batch-size', type=int, default=8,\n",
        "                        metavar='BS', help='batch size')\n",
        "    parser.add_argument('--epochs', type=int, default=40,\n",
        "                        metavar='E', help='number of epochs')\n",
        "    parser.add_argument('--class-weight', action='store_true',\n",
        "                        default=False, help='use class weights')\n",
        "    parser.add_argument('--active-listener', action='store_true',\n",
        "                        default=False, help='active listener')\n",
        "    parser.add_argument('--attention', default='simple',\n",
        "                        help='Attention type in context GRU')\n",
        "    parser.add_argument('--tensorboard', action='store_true',\n",
        "                        default=False, help='Enables tensorboard log')\n",
        "    parser.add_argument('--mode1', type=int, default=2,\n",
        "                        help='Roberta features to use')\n",
        "    parser.add_argument('--seed', type=int, default=100,\n",
        "                        metavar='seed', help='seed')\n",
        "    parser.add_argument('--norm', type=int, default=0,\n",
        "                        help='normalization strategy')\n",
        "    parser.add_argument('--mu', type=float, default=0, help='class_weight_mu')\n",
        "    parser.add_argument('--classify', default='emotion')\n",
        "    parser.add_argument('--residual', action='store_true',\n",
        "                        default=False, help='use residual connection')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    print(args)\n",
        "\n",
        "    args.cuda = torch.cuda.is_available() and not args.no_cuda\n",
        "    if args.cuda:\n",
        "        print('Running on GPU')\n",
        "    else:\n",
        "        print('Running on CPU')\n",
        "\n",
        "    if args.tensorboard:\n",
        "        from tensorboardX import SummaryWriter\n",
        "        writer = SummaryWriter()\n",
        "\n",
        "    emo_gru = True\n",
        "    if args.classify == 'emotion':\n",
        "        n_classes = 7\n",
        "    elif args.classify == 'sentiment':\n",
        "        n_classes = 3\n",
        "    cuda = args.cuda\n",
        "    n_epochs = args.epochs\n",
        "    batch_size = args.batch_size\n",
        "\n",
        "    global D_s\n",
        "\n",
        "    D_m = 1024\n",
        "    D_s = 768\n",
        "    D_g = 150\n",
        "    D_p = 150\n",
        "    D_r = 150\n",
        "    D_i = 150\n",
        "    D_h = 100\n",
        "    D_a = 100\n",
        "\n",
        "    D_e = D_p + D_r + D_i\n",
        "\n",
        "    global seed\n",
        "    seed = args.seed\n",
        "    # seed_everything(seed)\n",
        "\n",
        "    model = CommonsenseGRUModel(D_m, D_s, D_g, D_p, D_r, D_i, D_e, D_h, D_a,\n",
        "                                n_classes=n_classes,\n",
        "                                listener_state=args.active_listener,\n",
        "                                context_attention=args.attention,\n",
        "                                dropout_rec=args.rec_dropout,\n",
        "                                dropout=args.dropout,\n",
        "                                emo_gru=emo_gru,\n",
        "                                mode1=args.mode1,\n",
        "                                norm=args.norm,\n",
        "                                residual=args.residual)\n",
        "\n",
        "    print('MELD COSMIC Model.')\n",
        "\n",
        "    if cuda:\n",
        "        model.cuda()\n",
        "\n",
        "    if args.classify == 'emotion':\n",
        "        if args.class_weight:\n",
        "            if args.mu > 0:\n",
        "                loss_weights = torch.FloatTensor(create_class_weight(args.mu))\n",
        "            else:\n",
        "                loss_weights = torch.FloatTensor([0.30427062, 1.19699616, 5.47007183, 1.95437696,\n",
        "                                                  0.84847735, 5.42461417, 1.21859721])\n",
        "            loss_function = MaskedNLLLoss(\n",
        "                loss_weights.cuda() if cuda else loss_weights)\n",
        "        else:\n",
        "            loss_function = MaskedNLLLoss()\n",
        "\n",
        "    else:\n",
        "        loss_function = MaskedNLLLoss()\n",
        "\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "\n",
        "    if args.classify == 'emotion':\n",
        "        lf = open('logs/cosmic_meld_emotion_logs.txt', 'a')\n",
        "    elif args.classify == 'sentiment':\n",
        "        lf = open('logs/cosmic_meld_sentiment_logs.txt', 'a')\n",
        "\n",
        "    train_loader, valid_loader, test_loader = get_MELD_loaders(batch_size=batch_size,\n",
        "                                                               classify=args.classify,\n",
        "                                                               num_workers=0)\n",
        "\n",
        "    valid_losses, valid_fscores = [], []\n",
        "    test_fscores, test_losses = [], []\n",
        "    best_loss, best_label, best_pred, best_mask = None, None, None, None\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "        start_time = time.time()\n",
        "        train_loss, train_acc, _, _, _, train_fscore, _ = train_or_eval_model(\n",
        "            model, loss_function, train_loader, e, optimizer, True)\n",
        "        valid_loss, valid_acc, _, _, _, valid_fscore, _ = train_or_eval_model(\n",
        "            model, loss_function, valid_loader, e)\n",
        "        test_loss, test_acc, test_label, test_pred, test_mask, test_fscore, attentions = train_or_eval_model(\n",
        "            model, loss_function, test_loader, e)\n",
        "\n",
        "        valid_losses.append(valid_loss)\n",
        "        valid_fscores.append(valid_fscore)\n",
        "        test_losses.append(test_loss)\n",
        "        test_fscores.append(test_fscore)\n",
        "\n",
        "        if args.tensorboard:\n",
        "            writer.add_scalar('test: accuracy/loss', test_acc/test_loss, e)\n",
        "            writer.add_scalar('train: accuracy/loss', train_acc/train_loss, e)\n",
        "\n",
        "        x = 'epoch: {}, train_loss: {}, acc: {}, fscore: {}, valid_loss: {}, acc: {}, fscore: {}, test_loss: {}, acc: {}, fscore: {}, time: {} sec'.format(\n",
        "            e+1, train_loss, train_acc, train_fscore, valid_loss, valid_acc, valid_fscore, test_loss, test_acc, test_fscore, round(time.time()-start_time, 2))\n",
        "\n",
        "        print(x)\n",
        "        lf.write(x + '\\n')\n",
        "\n",
        "    if args.tensorboard:\n",
        "        writer.close()\n",
        "\n",
        "    valid_fscores = np.array(valid_fscores).transpose()\n",
        "    test_fscores = np.array(test_fscores).transpose()\n",
        "\n",
        "    score1 = test_fscores[0][np.argmin(valid_losses)]\n",
        "    score2 = test_fscores[0][np.argmax(valid_fscores[0])]\n",
        "    scores = [score1, score2]\n",
        "    scores = [str(item) for item in scores]\n",
        "\n",
        "    print('Test Scores: Weighted F1')\n",
        "    print('@Best Valid Loss: {}'.format(score1))\n",
        "    print('@Best Valid F1: {}'.format(score2))\n",
        "\n",
        "    if args.classify == 'emotion':\n",
        "        rf = open('results/cosmic_meld_emotion_results.txt', 'a')\n",
        "    elif args.classify == 'sentiment':\n",
        "        rf = open('results/cosmic_meld_sentiment_results.txt', 'a')\n",
        "\n",
        "    rf.write('\\t'.join(scores) + '\\t' + str(args) + '\\n')\n",
        "    rf.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}